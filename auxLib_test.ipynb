{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import random\n",
    "import os\n",
    "import time\n",
    "\n",
    "import scienceplots\n",
    "plt.style.use(['science', 'notebook'])\n",
    "\n",
    "#from tensorflow import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, Conv2DTranspose\n",
    "#from keras import models\n",
    "#from keras import layers\n",
    "\n",
    "import h5py\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Synthetic images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SyntheticImagesGen:\n",
    "    '''\n",
    "    Some examples on how to use SyntheticImagesGen class.\n",
    "\n",
    "    Initializing the class:\n",
    "    data = SyntheticImagesGen(10, training=['ferro','neel','stripe'], L=40)\n",
    "\n",
    "    Generating synthetic data:\n",
    "    train_images, train_labels = data.dataGenerator()\n",
    "\n",
    "    Info:    \n",
    "    data.Info()\n",
    "    '''\n",
    "\n",
    "    def __init__(self, number_configs: int, training: list[str]=['all'], L=40):\n",
    "        if training == ['all']:\n",
    "            training = ['para', 'ferro', 'neel', 'stripe']\n",
    "        else:\n",
    "            training = [element.lower() for element in training]\n",
    "\n",
    "        self.number_configs = number_configs\n",
    "        self.training = training\n",
    "        self.L = L\n",
    "\n",
    "    def spin_gen(self, conf: str):\n",
    "        '''\n",
    "        Generates a spin configuration for the given configuration. \n",
    "        The resulting data is a tuple of the different configurations each one can have.\n",
    "        For example, ferromagnetic configurations can be spin up ferromagnetic or spin down ferromagnetic.\n",
    "        In this case, the tuple will contain two LxL matrices, one per each type.\n",
    "        '''\n",
    "        spin_conf = []\n",
    "        if conf == 'ferro':\n",
    "            spin_conf = [np.ones((self.L, self.L)).astype(int), \n",
    "                        -np.ones((self.L, self.L)).astype(int)]\n",
    "        elif conf == 'neel':\n",
    "            spin = np.fromfunction(lambda i, j: (-1)**(i + j + (conf == 4)), \n",
    "                                (self.L, self.L)).astype(int)\n",
    "            spin_conf = [spin, -spin]\n",
    "        elif conf == 'stripe':\n",
    "            spin = np.fromfunction(lambda i, j: (-1)**j, (self.L, self.L)).astype(int)\n",
    "            spin_conf = [spin, -spin, spin.T, -spin.T]\n",
    "        elif conf == 'para':\n",
    "            spin_conf = np.random.choice([-1, 1], size=(self.L, self.L)).astype(int)\n",
    "        return spin_conf\n",
    "    \n",
    "    def Info(self):\n",
    "        return print(f'Number of configurations: {self.number_configs}\\nTraining: {self.training} \\nL: {self.L}\\n')\n",
    "\n",
    "    def dataGenerator(self):\n",
    "        ''' \n",
    "        Generates synthetic data given a number of configurations and the type of training we want to do.\n",
    "        If 'all', then it will generate all possible configurations evenly distributed. There are 4 types of configurations.\n",
    "        If the number of configurations is not divisible by 4, the remaining configurations will be generated in a paramagnetic manner.\n",
    "        '''\n",
    "        start_time = time.time()\n",
    "        print(\"Generating synthetic data...\")\n",
    "        config_dict = {\n",
    "            'para': 1,\n",
    "            'ferro': 2,\n",
    "            'neel': 2,\n",
    "            'stripe': 4\n",
    "        }\n",
    "\n",
    "        labels_dict = {\n",
    "            'para': 0,\n",
    "            'ferro': 1,\n",
    "            'neel': 2,\n",
    "            'stripe': 3\n",
    "        }\n",
    "\n",
    "        selected_dict = {k: v for k, v in config_dict.items() if k in self.training}\n",
    "\n",
    "        selected_labels = {k: v for k, v in labels_dict.items() if k in self.training}\n",
    "        \n",
    "        total_configs_per_selected = self.number_configs // len(selected_dict.values())\n",
    "        remaining_configs = self.number_configs % len(selected_dict.values())\n",
    "\n",
    "        train_images = []\n",
    "        train_labels = []\n",
    "\n",
    "        if 'para' in self.training:\n",
    "            for _ in range(total_configs_per_selected):\n",
    "                train_images.append(self.spin_gen('para'))\n",
    "                train_labels.append(labels_dict['para'])\n",
    "            del selected_dict['para'], selected_labels['para']\n",
    "        \n",
    "        for conf in selected_dict:\n",
    "            total_conf = total_configs_per_selected\n",
    "            extra_configs = total_conf % selected_dict[conf]    \n",
    "                \n",
    "            if extra_configs !=0:\n",
    "                remaining_configs += extra_configs\n",
    "            \n",
    "            total_conf = total_conf // selected_dict[conf]\n",
    "\n",
    "            for _ in range(total_conf):\n",
    "                train_images.extend(self.spin_gen(conf))\n",
    "                for _ in range(config_dict[conf]):\n",
    "                    train_labels.append(labels_dict[conf])\n",
    "\n",
    "        if 'para' in self.training:\n",
    "            for _ in range(remaining_configs):\n",
    "                train_images.append(self.spin_gen('para'))\n",
    "                train_labels.append(labels_dict['para'])\n",
    "        else:\n",
    "            for i in range(remaining_configs):\n",
    "                index = i % len(self.training)\n",
    "                train_images.append(self.spin_gen(self.training[index])[0])\n",
    "                train_labels.append(labels_dict[self.training[index]])\n",
    "\n",
    "        temp = list(zip(train_images, train_labels))\n",
    "        random.shuffle(temp)\n",
    "        train_images, train_labels = zip(*temp)\n",
    "\n",
    "        print(\"Done!\")\n",
    "\n",
    "        end_time = time.time()\n",
    "        elapsed_time = end_time - start_time\n",
    "        print(\"Elapsed time:\", elapsed_time, \"seconds\")\n",
    "        return np.array(train_images), np.array(train_labels)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simulated images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class loader_and_saver:\n",
    "    '''\n",
    "    Some examples on how to use the loader_and_saver class.\n",
    "\n",
    "    Initializing the class:\n",
    "    loading_data = loader_and_saver(os.getcwd())\n",
    "\n",
    "    Saving data: Given some data set, in this case 'train_images'.\n",
    "    loading_data.saver(train_images)\n",
    "\n",
    "    Loading simulated images:\n",
    "    sim_images, temperature = loading_data.simulatedImages(5)\n",
    "\n",
    "    Loading data from the loader given some os path:\n",
    "    sim = loading_data.loader(os.path.join(os.getcwd(),'2024-08-10','data_2'))\n",
    "\n",
    "    Using the checker:\n",
    "    loader_and_saver.checker(train_images, sim)\n",
    "    '''\n",
    "    def __init__(self, path):\n",
    "        self.path = path\n",
    "    \n",
    "    \n",
    "    def saver(self, data, directory=None, name_of_file='data'):\n",
    "        if directory is None:\n",
    "            directory = datetime.now().strftime('%Y-%m-%d')\n",
    "\n",
    "        if not os.path.exists(directory):\n",
    "            os.makedirs(directory)\n",
    "\n",
    "        base_name = name_of_file.strip()\n",
    "        existing_files = [f for f in os.listdir(directory) if f.startswith(base_name) and f.endswith('.h5')]\n",
    "        name_suffix = len(existing_files) + 1\n",
    "        name = f\"{base_name}_{name_suffix}.h5\"\n",
    "\n",
    "        file_path = os.path.join(self.path,directory,name)\n",
    "        \n",
    "        with h5py.File(file_path, 'w') as f:\n",
    "            for i, arr in enumerate(tqdm(data, desc=\"Saving images\", unit=\"array\")):\n",
    "                f.create_dataset(f'array_{i}', data=arr, compression='gzip', compression_opts=9)\n",
    "        print(\"Files saved as\", file_path)\n",
    "\n",
    "\n",
    "    def loader(self, file_name):\n",
    "        name = file_name\n",
    "        if name[:-3] !='.h5':\n",
    "            name += '.h5'\n",
    "\n",
    "        loaded_list = []\n",
    "        with h5py.File(name, 'r') as f:\n",
    "            for key in tqdm(sorted(f.keys(), key=lambda x: int(x.split('_')[1])), \n",
    "                            desc=\"Loading arrays\", unit=\"array\"):\n",
    "                loaded_list.append(f[key][:])\n",
    "        print(\"Files loaded!\")\n",
    "        return loaded_list\n",
    "\n",
    "\n",
    "    def checker(original_list, loaded_list):\n",
    "        data_is_equal = True\n",
    "        for original, loaded in zip(original_list, loaded_list):\n",
    "            if not np.array_equal(original, loaded):\n",
    "                data_is_equal = False\n",
    "                break\n",
    "        if data_is_equal:\n",
    "            print(\"The original data set and the loaded data set are identical.\")\n",
    "        else:\n",
    "            print(\"The original data set and the loaded data set are NOT identical.\")\n",
    "\n",
    "\n",
    "    def simulatedImages(self, index: int):\n",
    "        print('Loading simulated images...')\n",
    "\n",
    "        densityIndices = ['055','06', '061', '062', '063', '064', '065', '07', '08', '09','1']\n",
    "\n",
    "        loadingPath = os.path.join(self.path,'data',f'data_p{densityIndices[index]}')\n",
    "        \n",
    "        simImages = self.loader(loadingPath)\n",
    "        \n",
    "        temperature = np.arange(0.0, 5.02, 0.02).tolist()\n",
    "        dens_format = densityIndices[index][:1]+'.'+densityIndices[index][1:]\n",
    "        print(f'Data of density p = {dens_format} succesfully loaded.')\n",
    "        \n",
    "        return simImages, temperature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Figures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def latticeGraph(squareLattice: list, size=40):\n",
    "    \"\"\"\n",
    "    Generates a graph of square lattices using the given list of square lattices.\n",
    "\n",
    "    Parameters:\n",
    "        squareLattice (list): A list of square lattices to be plotted.\n",
    "        size (int, optional): The size of the square lattices. Defaults to 40.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    cmap1 = ListedColormap(['white', 'gray', 'black'])\n",
    "    \n",
    "    numPlots = len(squareLattice)\n",
    "    rows = int(np.ceil(numPlots / 3))\n",
    "    cols = min(3, numPlots)  # Ensure cols is at most 3\n",
    "\n",
    "    fig = plt.figure(figsize=(10*cols, 10*rows))\n",
    "    gs = gridspec.GridSpec(rows, cols + 1, width_ratios=[1]*cols + [0.05])\n",
    "\n",
    "    ax = [fig.add_subplot(gs[i, j]) for i in range(rows) for j in range(cols)]\n",
    "    \n",
    "    for i, axi in enumerate(ax):\n",
    "        if i < numPlots:\n",
    "            im1 = axi.imshow(squareLattice[i], cmap=cmap1,\n",
    "                            interpolation='nearest', vmin=-1, vmax=1)\n",
    "            axi.set_xticks(np.arange(-0.5, size, 5))\n",
    "            axi.set_yticks(np.arange(-0.5, size, 5))\n",
    "            axi.set_xticklabels([])\n",
    "            axi.set_yticklabels([])\n",
    "        else:\n",
    "            fig.delaxes(axi)  # Remove empty subplots\n",
    "\n",
    "    # Add colorbar to the right of the entire figure\n",
    "    cbar_ax = fig.add_subplot(gs[:, -1])\n",
    "    fig.colorbar(im1, cax=cbar_ax, ticks=[-1, 0, 1])\n",
    "\n",
    "    # Display the plot\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    return\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural networks generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DenseNeuralNetworkGen:\n",
    "    def __init__(self, input_shape, num_classes, layers=None):\n",
    "        self.model = Sequential()\n",
    "        if layers is None:\n",
    "            layers = [\n",
    "                {'type': 'conv', 'filters': 32, 'kernel_size': 3, 'activation': 'relu', 'pool_size': 2},\n",
    "                {'type': 'conv', 'filters': 64, 'kernel_size': 3, 'activation': 'relu', 'pool_size': 2},\n",
    "                {'type': 'flatten'},\n",
    "                {'type': 'dense', 'units': 128, 'activation': 'relu'},\n",
    "                {'type': 'dense', 'units': num_classes, 'activation': 'softmax'}\n",
    "            ]\n",
    "        for layer in layers:\n",
    "            if layer['type'] == 'conv':\n",
    "                self.model.add(Conv2D(layer['filters'], layer['kernel_size'], \n",
    "                                    activation=layer['activation'], input_shape=input_shape))\n",
    "                if layer['pool_size'] is not None:\n",
    "                    self.model.add(MaxPooling2D(pool_size=layer['pool_size']))\n",
    "            elif layer['type'] == 'flatten':\n",
    "                self.model.add(Flatten())\n",
    "            elif layer['type'] == 'dense':\n",
    "                self.model.add(Dense(layer['units'], activation=layer['activation']))\n",
    "            elif layer['type'] == 'dropout':\n",
    "                self.model.add(Dropout(layer['rate']))\n",
    "            elif layer['type'] == 'convTranspose':\n",
    "                self.model.add(Conv2DTranspose(1, layer['kernel_size'],\n",
    "                                                    strides=layer['strides'], \n",
    "                                                    padding=layer['padding'], \n",
    "                                                    activation=layer['activation']))\n",
    "\n",
    "    def compile(self, optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy']):\n",
    "        self.model.compile(optimizer=optimizer, loss=loss, metrics=metrics)\n",
    "\n",
    "    def fit(self, x_train, y_train, epochs=10, batch_size=32, validation_data=None):\n",
    "        self.model.fit(x_train, y_train, epochs=epochs, batch_size=batch_size, validation_data=validation_data)\n",
    "\n",
    "    def evaluate(self, x_test, y_test):\n",
    "        return self.model.evaluate(x_test, y_test)\n",
    "\n",
    "    def predict(self, x):\n",
    "        return self.model.predict(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "cnn = DenseNeuralNetworkGen((28, 28, 1), 10, layers=[\n",
    "    {'type': 'conv', 'filters': 32, 'kernel_size': 3, 'activation': 'relu', 'pool_size': 2},\n",
    "    {'type': 'conv', 'filters': 64, 'kernel_size': 3, 'activation': 'relu', 'pool_size': 2},\n",
    "    {'type': 'flatten'},\n",
    "    {'type': 'dense', 'units': 128, 'activation': 'relu'},\n",
    "    {'type': 'dropout', 'rate': 0.5},\n",
    "    {'type': 'dense', 'units': 10, 'activation': 'softmax'},\n",
    "    {'type': 'conv', 'filters': 64, 'kernel_size': new_filt, 'strides':  (1, 1),'padding': 'valid', 'activation': 'relu'}\n",
    "])\n",
    "cnn.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "cnn.fit(x_train, y_train, epochs=15, batch_size=64, validation_data=(x_test, y_test))\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
